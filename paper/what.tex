From quantum mechanics, we learn that the world is defined by Hilbert spaces
that evolve according to unitary transformations.\citeme{find something}

\begin{definition}[Hilbert space]\label{def:hilbert-space}
  \asz{This is a (slightly modified) quote from the Wikipedia article on Hilbert
  spaces; we need something better.}  ``A \term{Hilbert space} $H$ is a complex
  inner product space that is also a complete metric space with respect to the
  distance function induced by the inner product.''
\end{definition}

\begin{definition}[Conjugate transpose]\label{def:conjugate-transpose}
  The \term{conjugate transpose} of a matrix $M = (M_{i,j}) \in \C^{m \cross
  n}$, written $M^\dag$, is given by taking the complex conjugate of each
  element of $M^\T$; in other words, $M^\dag = (\overline{M_{j,i}})$.
\end{definition}

\begin{definition}[Hermitian matrix]\label{def:hermitian}
  A matrix $M$ is \term{Hermitian} if $M = M^\dag$.
\end{definition}

\begin{definition}[Unitary matrix]\label{def:unitary-matrix}
  A \term{unitary matrix} is a square matrix $U \in \C^{n \cross n}$ such that
  $U^\dag U = I$.\asz{Wikipedia defines a unitary \emph{transformation} as an
  isomorphism of Hilbert spaces, or a linear transformation that preserves the
  inner product.  Should we mention this?}
\end{definition}

We also need to briefly touch on notation.  We adopt the physicists' convention
of writing $\ket{\psi}$ for an element of a Hilbert space $H$; this is referred
to as a \term{ket}.  The dual of a ket is a \term{bra}, written $\bra{\phi}$,
which is an element of $H$'s dual space.  When working with vectors in $\C^n$,
as we will be doing, we can define $\bra{\psi} = \ket{\psi}^\dag$: where kets
are column vectors, bras are row vectors.  The notation $\braket{\phi | \psi}$
then denotes the inner product of $\ket{\phi}$ and $\ket{\psi}$, which is just
matrix multiplication in the case of $\C^n$.  Importantly, this means that we
have now also adopted the physicist's convention that the inner product is
linear in its \emph{second} argument, and conjugate-linear in its \emph{first}.

Fortunately, for quantum computation, we are only interested in very particular
Hilbert spaces.  These are the spaces that arise from considering the single
fundamental primitive datum of quantum computation: the \term{qubit}.  A qubit
is a \emph{qu}antum \emph{b}it: when we have a classical bit, we know that it is
either $0$ or $1$; similarly, when we measure a qubit, we know that we will get
either $\ket{0}$ or $\ket{1}$.  The difference between a qubit and a bit lies in
the word ``measure'': as long as we \emph{don't} measure a qubit, it can take on
all sorts of crazy states.  Formally, we can represent a qubit as an element of
$\C^2$ subject to some normalization constraints:

\begin{definition}[Qubit]\label{def:qubit}
  A \term{qubit} is a vector of two complex numbers \[ \vector{a b} \in \C^2 \]
  such that $|a|^2 + |b|^2 = 1$.  We define $\ket{0}$ and $\ket{1}$ to be the
  basis vectors \[ \ket{0} = \vector{1 0} \text{ and } \ket{1} = \vector{0
  1}, \] and will thus often write the above qubit as $a\ket{0} + b\ket{1}$.  We
  call $a$ and $b$ \term{probability amplitudes}, for reasons which will become
  clear later.
\end{definition}

Now that we have this definition, we can define our first unitary operator:
bitwise negation, which we define as \[
  X = \begin{pmatrix} 0 & 1 \\
                      1 & 0 \end{pmatrix}.
\] Seeing that $X$ is unitary is straightforward, since it is both self-inverse
and Hermitian.  It is also easy to see that $X\ket{0} = \ket{1}$ and $X\ket{1} =
\ket{0}$, justifying the name bitwise negation.  But qubits are more than just
zeroes and ones.  If we apply bitwise negation to an arbitrary qubit $a\ket{0} +
b\ket{1}$, we get \[
  X(a\ket{0} + b\ket{1}) =
  \begin{pmatrix}
    0 & 1 \\
    1 & 0
  \end{pmatrix}
  \vector{a b} =
  \vector{b a} =
  b\ket{0} + a\ket{1}.
\]  Thus, bitwise negation swaps the probability amplitudes of an arbitrary
qubit.

\begin{figure}
  \centerline{\Qcircuit{
    \lstick{\ket{0}} & \gate{X} & \rstick{\ket{1}} \qw
  }}
  \caption{The quantum circuit diagram for applying the bitwise not operator to
    a single qubit; here, it is applied to the value $\ket{0}$.}
  \label{qcd:bitwise-not-0}
\end{figure}

We can also represent applying the unitary transformation $X$ diagrammatically
through the use of a \term{quantum circuit diagram}, such as we see in
\cref{qcd:bitwise-not-0}.  The horizontal lines are \term{wires}, and are read
from left to right.  A box which spans some wires and contains a term is a
\term{gate}; the effect of this gate is given by the unitary matrix within the
box, and the whole thing is to be read as applying that matrix to the given
qubits.\footnotemark{}  The qubit labels at the left ends of the wires are the
input values; the qubit labels at the right end are the output values.
Sometimes, we will give a qubit a name as well as an initial value; the name
will be placed even further left.

\footnotetext{Our mathematically inclined readers might notice here that this is
  reminiscent of category theoretic diagrams, including composition happening in
  diagrammatic order.  This is indeed the case, which will be explained in more
  detail in \cref{sec:how}.}

This brings us nicely to the next property we must discuss: we talk above about
wire\emph{s}, plural.  But we haven't yet said how to define multi-qubit states!
Given two qubits $\ket{a}$ and $\ket{b}$, where $\ket{a} = a_0\ket{0} +
a_1\ket{1}$ and $\ket{b} = b_0\ket{0} + b_1\ket{1}$, how can we talk about the
system consisting of both $\ket{a}$ and $\ket{b}$?  To those of us used to the
traditional Newtonian worldview, this is an odd question; can't we just use the
pair $(\ket{a},\ket{b})$?  After all, vector spaces are closed under the
Cartesian product.

But here, too, quantum mechanics departs from our intuitions.  In quantum
mechanics, when we combine two state spaces, we take their \emph{tensor} product
instead.  For a fully formal definition of the tensor product \asz{Because \emph{I}
certainly can't define it.}, consult your favorite textbook on linear algebra,
such as \citeme{I dunno}; however, we define the relevant operations for the
finite-dimensional vector spaces that we care about.

\begin{definition}[Tensor product]\label{def:tensor-product}
  The \term{tensor product of vectors} $\ket{\phi} \in \C^m$ and $\ket{\psi} \in
  \C^n$ with zero-indexed entries $\phi_i$ and $\psi_j$, respectively, is the
  vector in $\C^{mn}$ whose entries are the pairwise products of the entries in
  $\ket{\phi}$ and the entries in $\ket{\psi}$.  More precisely, we have \[
    \ket{\phi} \tensor \ket{\psi} =
    \vector{\phi_0\ket{\psi} \phi_1\ket{\psi} \vdots{} \phi_{m-1}\ket{\psi}} =
    \vector{\phi_0\psi_0 \phi_0\psi_1 \vdots{} \phi_0\psi_{n-1}
            \phi_1\psi_0 \phi_1\psi_1 \vdots{} \phi_1\psi_{n-1}
            \vdots{}
            \phi_{m-1}\psi_0 \phi_{m-1}\psi_1 \vdots{} \phi_{m-1}\psi_{n-1}}
    \in \C^{mn}.
  \]  We also write this as $\ket{\psi}\ket{\phi}$ or $\ket{\psi\phi}$ if there
  will be no ambiguity.
  
  % "Internal" to the submatrix -- spacing is hard to come by here.
  \def\icdots{\mathclap{\cdots}}
  \def\iddots{\mathclap{\ddots}}
  The \term{tensor product of matrices} $U \in \C^{m \cross n}$ and $V \in \C^{p
  \cross q}$ with one-indexed\footnotemark{} entries $u_{ij}$ and $v_{ij}$,
  respectively, is the $mp \cross nq$ matrix whose entries are the pairwise
  products of the entries in $U$ and the entries in $V$.  More precisely, we
  have \begin{gather*}
    U \tensor V =
    \begin{pmatrix}
      u_{11}V & u_{21}V & \cdots & u_{m1}V \\
      u_{12}V & u_{22}V & \cdots & u_{m2}V \\
      \vdots & \vdots & \ddots & \vdots \\
      u_{1n}V & u_{2n}V & \cdots & u_{mn}V
    \end{pmatrix} \\ =
    \left(\begin{array}{@{}*{2}{c@{\;\;}ccc}cc@{\;\;}ccc@{}}
      u_{11}v_{11} & u_{11}v_{21} & \icdots & u_{11}v_{p1} &
      u_{21}v_{11} & u_{21}v_{21} & \icdots & u_{21}v_{p1} &
      \cdots &
      u_{m1}v_{11} & u_{m1}v_{21} & \icdots & u_{m1}v_{p1}
      \\
      u_{11}v_{12} & u_{11}v_{22} & \icdots & u_{11}v_{p2} &
      u_{21}v_{12} & u_{21}v_{22} & \icdots & u_{21}v_{p2} &
      \cdots &
      u_{m1}v_{12} & u_{m1}v_{22} & \icdots & u_{m1}v_{p2}
      \\
      \vdots & \vdots & \iddots & \vdots &
      \vdots & \vdots & \iddots & \vdots &
      \cdots &
      \vdots & \vdots & \iddots & \vdots
      \\
      u_{11}v_{1q} & u_{11}v_{2q} & \icdots & u_{11}v_{pq} &
      u_{21}v_{1q} & u_{21}v_{2q} & \icdots & u_{21}v_{pq} &
      \cdots &
      u_{m1}v_{1q} & u_{m1}v_{2q} & \icdots & u_{m1}v_{pq}
      \\
      \vdots & \vdots & \vdots & \vdots &
      \vdots & \vdots & \vdots & \vdots &
      \ddots &
      \vdots & \vdots & \vdots & \vdots
      \\
      u_{1n}v_{11} & u_{1n}v_{21} & \icdots & u_{1n}v_{p1} &
      u_{2n}v_{11} & u_{2n}v_{21} & \icdots & u_{2n}v_{p1} &
      \cdots &
      u_{mn}v_{11} & u_{mn}v_{21} & \icdots & u_{mn}v_{p1}
      \\
      u_{1n}v_{12} & u_{1n}v_{22} & \icdots & u_{1n}v_{p2} &
      u_{2n}v_{12} & u_{2n}v_{22} & \icdots & u_{2n}v_{p2} &
      \cdots &
      u_{mn}v_{12} & u_{mn}v_{22} & \icdots & u_{mn}v_{p2}
      \\
      \vdots & \vdots & \iddots & \vdots &
      \vdots & \vdots & \iddots & \vdots &
      \cdots &
      \vdots & \vdots & \iddots & \vdots
      \\
      u_{1n}v_{1q} & u_{1n}v_{2q} & \icdots & u_{1n}v_{pq} &
      u_{2n}v_{1q} & u_{2n}v_{2q} & \icdots & u_{2n}v_{pq} &
      \cdots &
      u_{mn}v_{1q} & u_{mn}v_{2q} & \icdots & u_{mn}v_{pq}
    \end{array}\right).% \\ \in \C^{mp \cross nq} % Add back in if there's space
  \end{gather*}

  \footnotetext{Solely for space reasons.}  
\end{definition}

